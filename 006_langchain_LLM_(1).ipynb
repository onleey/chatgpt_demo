{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onleey/chatgpt_demo/blob/master/006_langchain_LLM_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "X5nSX1hGYR1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd '/content/drive/MyDrive/001'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPjGqwqKHR_W",
        "outputId": "55b8c40b-4f5e-4b74-a84c-a182bfb0b263"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQ2KGyLJ99Yi"
      },
      "source": [
        "# 랭체인 사전 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 랭체인이란?\n",
        "-  LLM을 활용한 애플리케이션 개발을 지원하는 오픈소스 라이브러리이다.\n",
        "- 랭체인이 도움이 되는 경우는 LLM에 외부의 '지식'이나 '계산능력'을 활용하게 하고 싶을때이다.\n",
        "- 자신이 학습만 것만으로 대화하던 LLM에게 '책'이나 '프로그램'을 전달해서 외부의 '지식'이나 '계산능력'을 활용할 수 있게 하는 것이 랭체인의 역할이다.\n",
        "- 예를 들어, 랭체인으로 LLM에 웹 검색 기능을 연결하면 LLM은 자신이 가진 지식만으로는 충분한 답변을 할 수 없는 경우에 웹 검색을 통해 최신 정보를 얻고 답변할 수 있게 된다.\n",
        "### 랭체인의 모듈\n",
        "랭체인은 LLM을 활용한 애플리케이션 개발에 도움이 되는 다양한 모듈을 제공한다. 모듈은 개별적으로 사용할 수 있을뿐더러 여러 개를 조합해서 복잡한 애플리케이션을 구축할 수도 있다.\n",
        "- LLM : LLM 호출을 위한 공통 인터페이스\n",
        "- 프롬프트 템플릿 : 사용자 입력에 따른 프롬프트 생성\n",
        "- 체인 : 여러 LLM과 프롬프트의 입출력을 연결\n",
        "- 에이전트 : 사용자의 요청에 따라 어떤 기능을 어떤 순서로 실행할 것인지 결정\n",
        "- 도구 : 에이전트가 수행하는 특정 기능\n",
        "- 메모리 : 체인 및 에이전트의 메모리 보유"
      ],
      "metadata": {
        "id": "zPd3CCxmR_ov"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mkPbwp6-ploX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0380a0a5-e954-439d-d881-b45b219881c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.354\n",
            "  Downloading langchain-0.0.354-py3-none-any.whl (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.3/803.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (3.9.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.354)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.354)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.8 (from langchain==0.0.354)\n",
            "  Downloading langchain_community-0.0.8-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.5 (from langchain==0.0.354)\n",
            "  Downloading langchain_core-0.1.6-py3-none-any.whl (208 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.77 (from langchain==0.0.354)\n",
            "  Downloading langsmith-0.0.77-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.354) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.354) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.354)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.354)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.354)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.5->langchain==0.0.354) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.5->langchain==0.0.354) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.354) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.354) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.354) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.354) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.354) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.354) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.5->langchain==0.0.354) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.5->langchain==0.0.354) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.354)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.3 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.354 langchain-community-0.0.8 langchain-core-0.1.6 langsmith-0.0.77 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m784.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ],
      "source": [
        "# 패키지 설치\n",
        "!pip install langchain==0.0.354\n",
        "!pip install openai==0.28.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_Gf8Rfw2MiG_"
      },
      "outputs": [],
      "source": [
        "# 환경변수 준비\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPJ2lQW0-J3a"
      },
      "source": [
        "# LLM\n",
        "- 랭체인은 다양한 LLM을 동일한 방식으로 이용할 수 있는 공통 인터페이스를 제공한다.\n",
        "- OpenAI 클래스는 공통 인터페이스의 LLM 클래스를 상속한 클래스로, 기본적으로 'text-davinci-003'이 사용된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YBYKjuiZBAXn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63d002e2-35f7-4fe0-bc5a-e693ee07903d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"픽셀팩토리\"\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# LLM 준비\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# LLM 호출\n",
        "print(llm(\"컴퓨터 게임을 만드는 새로운 한국어 회사명을 하나 제안해 주세요\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5eQoz9kBBQs"
      },
      "source": [
        "# 프롬프트 템플릿\n",
        "- 프롬프트 템플릿은 사용자 입력으로 프롬프트를 생성하기 위한 템플릿이다.\n",
        "- LLM을 이용한 애플리케이션을 개발할 때 일반적으로 사용자 입력을 직접 LLM에 전달하는 경우는 많지 않다. 대부분 좋은 답변을 반환하는 것으로 확인된 프롬프트 구문에 사용자 입력을 삽입한후 LLM에 전달하는 경우가 많다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nMYoyMqqBAZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ea31ba3-68e2-4233-b2be-3aa49e4b1542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "가정용 로봇을 만드는 새로운 한국어 회사명을 하나 제안해 주세요\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# 프롬프트 템플릿 만들기\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"{product}을 만드는 새로운 한국어 회사명을 하나 제안해 주세요\",\n",
        ")\n",
        "\n",
        "# 프롬프트 생성\n",
        "print(prompt.format(product=\"가정용 로봇\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOx3wED7CIwt"
      },
      "source": [
        "# 체인\n",
        "- 체인은 여러 개의 LLM이나 프롬프트의 입출력을 연결하기 위한 모듈이다.\n",
        "- 앞의 예제처럼 LLM과 프롬프트 템플릿을 단독으로 사용했지만 실제 애플리케이션에서는 이를 체인으로 묶어 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m1gZNBQ2BAbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba084d11-2566-4b62-e340-dcc21875a1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"로보샤\"\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# 프롬프트 템플릿 만들기\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"{product}을 만드는 새로운 한국어 회사명을 하나 제안해 주세요\",\n",
        ")\n",
        "\n",
        "# 체인 생성\n",
        "chain = LLMChain(\n",
        "    llm=OpenAI(temperature=0.9),\n",
        "    prompt=prompt\n",
        ")\n",
        "\n",
        "# 체인 실행\n",
        "#strip()을 이용하여 줄바꿈 문자 제거\n",
        "print(chain.run(\"가정용 로봇\").strip())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 랭체인은 LLM(대규모 언어 모델)을 다양한 용도로 활용할 수 있도록 하는 라이브러리이다.\n",
        "# LLM이란\n",
        "- 랭체인은 LLM클래스는 LLM 호출을 위한 공통 인터페이스이다.  이를 통해 애플리케이션에서 사용하는 LLM을 손쉽게 전환할 수 있다.\n",
        "- 그러나 랭체인의 능력을 충분히 활용할 수 있는 만큼의 언어 이해 능력을 가진 LLM은 적고, 보통 GPT-4, GPT-3.5 로 한정돼 있다.\n",
        "- LLM 클래스 종류\n",
        " - OpenAI클래스 : 텍스트 생성 모델(text-davinci-003)\n",
        " - ChatOpenAI 클래스 : 채팅 모델(gpt-3.5-turbo/gpt-4)\n",
        "\n",
        "- 랭체인에서 제공하는 LLM목록\n",
        "https://integrations.langchain.com/llms"
      ],
      "metadata": {
        "id": "ecRPs-mDcC_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 텍스트 생성 모델의 LLM 호출\n",
        "- 텍스트 생성 모델(text-davinci-003)의 LLM 호출은 OpenAI클래스를 사용한다.\n",
        "- OpenAI( )의 주요 매개변수\n",
        "\n",
        "|매개변수|설명|\n",
        "|---|---|\n",
        "|model_name|OpenAI의 모델ID(text-davinci-003)|\n",
        "|max_tokens|최대 출력 토큰수|\n",
        "|temperature|무작위성, 창의성을 발휘하게 하려면 0.7, 답이 있는 경우 0을 권장|\n",
        "|n|생성할 결과 수(기본값:1)|\n",
        "|cache|캐시 활성화/비활성화|\n",
        "|streaming|스트리밍 활성화/비활성화|\n",
        "|callback_manager|콜백매니저|"
      ],
      "metadata": {
        "id": "qeHSWZleb_Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "# LLM 준비\n",
        "llm = OpenAI(\n",
        "    model_name=\"text-davinci-003\",  # 모델 ID\n",
        "    temperature=0  # 무작위성\n",
        ")"
      ],
      "metadata": {
        "id": "WRTtqTvecAFD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLM 호출을 하기 위해서는 LLM클래스의 `__call__( )`을 사용한다. `__call__()` 은 변수 자체를 함수처럼 호출하는 파이썬 기능이다. llm변수명의 경우 llm( )으로 실행한다."
      ],
      "metadata": {
        "id": "0aivwq_9gp77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM 호출\n",
        "result = llm(\"고양이 울음소리는?\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUQwuSWAfJN-",
        "outputId": "6eafe2d2-ad65-4993-800a-8b0512a5d758"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "고양이 울음소리는 \"야옹\"으로 나타납니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "고급 LLM 호출을 위해서는 LLM클래스의 generation()을 사용한다. 여러 텍스트를 종합적으로 추론할 수 있으며, 사용한 토큰 수와 같은 정보를 얻을 수 있다."
      ],
      "metadata": {
        "id": "OQoSENn6hC7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 고급 LLM 호출\n",
        "result = llm.generate([\"고양이 울음소리는?\", \"까마귀 울음소리는?\"])\n",
        "print(result)\n",
        "\n",
        "# 출력 텍스트\n",
        "print(\"response0:\", result.generations[0][0].text)\n",
        "print(\"response1:\", result.generations[1][0].text)\n",
        "\n",
        "# 사용한 토큰 개수\n",
        "print(\"llm_output:\", result.llm_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKJOEeQmfNEZ",
        "outputId": "8c96ccb3-2258-4155-f684-905d5b284965"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generations=[[Generation(text='\\n\\n고양이 울음소리는 \"야옹\"으로 나타납니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text='\\n\\n까마귀의 울음소리는 \"까악까악\"이라고 합니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]] llm_output={'token_usage': {'completion_tokens': 109, 'prompt_tokens': 47, 'total_tokens': 156}, 'model_name': 'text-davinci-003'} run=[RunInfo(run_id=UUID('996d81e0-541d-45ea-b8ce-c4e588d5f3b6')), RunInfo(run_id=UUID('0d7ead7a-45a4-4c29-a89a-95ba7c6261a8'))]\n",
            "response0: \n",
            "\n",
            "고양이 울음소리는 \"야옹\"으로 나타납니다.\n",
            "response1: \n",
            "\n",
            "까마귀의 울음소리는 \"까악까악\"이라고 합니다.\n",
            "llm_output: {'token_usage': {'completion_tokens': 109, 'prompt_tokens': 47, 'total_tokens': 156}, 'model_name': 'text-davinci-003'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 채팅 모델의 LLM 호출\n",
        "\n",
        "### LLM 준비\n",
        "- 채팅 모델(gpt-3.5-turbo/gpt-4)의 LLM 호출에는 ChatOpenAI 클래스를 사용한다.\n",
        "- ChatOpenAI():의 주요 매개변수\n",
        "\n",
        "|매개변수|설명|\n",
        "|---|---|\n",
        "|model_name|ChatOpenAI의 모델ID(gpt-3.5-turbo/gpt-4)|\n",
        "|max_tokens|최대 출력 토큰수|\n",
        "|temperature|무작위성, 창의성을 발휘하게 하려면 0.7, 답이 있는 경우 0을 권장|\n",
        "|n|생성할 결과 수(기본값:1)|\n",
        "|cache|캐시 활성화/비활성화|\n",
        "|streaming|스트리밍 활성화/비활성화|\n",
        "|callback_manager|콜백매니저|"
      ],
      "metadata": {
        "id": "_Dc1JRJghsQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# LLM 준비\n",
        "chat_llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",  # 모델 ID\n",
        "    temperature=0  # 무작위성\n",
        ")"
      ],
      "metadata": {
        "id": "sg8xO2l7huCA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM 호출\n",
        "- LLM 호출을 하기 위해서는 LLM클래스의 `__call__( )`을 사용한다. `__call__()` 은 변수 자체를 함수처럼 호출하는 파이썬 기능이다. chat_llm변수명의 경우 chat_llm( )으로 실행한다.\n",
        "- ChatOpenAI 클래스의 LLM 호출에서는 채팅 메시지 리스트를 매개변수로 전달한다. 채팅 메시지 리스트는 SystemMessage, HumanMessage, AIMessage 중 하나를 요소로 하는 배열이다. 이것들은 OpenAI API의 system, user, assistane에 해당한다.\n",
        "  - SystemMessage :  시스템 메시지(system)\n",
        "  - HumanMessage : 인간 메시지(user)\n",
        "  - AIMessage : AI 메시지(assistant)"
      ],
      "metadata": {
        "id": "zr3hjxHCjUgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "# LLM 호출\n",
        "messages = [\n",
        "    HumanMessage(content=\"고양이 울음소리는?\")\n",
        "]\n",
        "result = chat_llm(messages)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpATpYvdjfpJ",
        "outputId": "45054c62-5399-413a-895b-e3ab922ae421"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='고양이의 울음소리는 \"야옹\"이라고 표현됩니다.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 고급 LLM 호출\n",
        "고급 LLM 호출을 하려면 LLM 클래스의 generation()을 사용한다. 여러 개의 채팅 메시지 리스트를 종합적으로 추론할 수 있고, 사용한 토큰 개수 등의 정보도 얻을 수 있다."
      ],
      "metadata": {
        "id": "v3P6GyyIkegS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 고급 LLM 호출\n",
        "messages_list = [\n",
        "    [HumanMessage(content=\"고양이 울음소리는?\")],\n",
        "    [HumanMessage(content=\"까마귀 울음소리는?\")]\n",
        "]\n",
        "result = chat_llm.generate(messages_list)\n",
        "\n",
        "# 출력 텍스트\n",
        "print(\"response0:\", result.generations[0][0].text)\n",
        "print(\"response1:\", result.generations[1][0].text)\n",
        "\n",
        "# 사용한 토큰 개수\n",
        "print(\"llm_output:\", result.llm_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkCyr-jYkW_p",
        "outputId": "faa656b6-04a0-4338-d67b-d7384aa824c2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response0: 고양이의 울음소리는 \"야옹\"이라고 표현됩니다.\n",
            "response1: 까악까악\n",
            "llm_output: {'token_usage': {'prompt_tokens': 38, 'completion_tokens': 34, 'total_tokens': 72}, 'model_name': 'gpt-3.5-turbo'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM 캐시\n",
        "- LLM의 입출력 텍스트를 캐싱해서 동일한 입력 텍스트로 LLM  호출이 발생했을 때 캐시를 활용할 수 있다. 이를 통해 속도 및 토큰 사용량을 줄일 수 있다.\n",
        "- 랭체인에는 4종류의 캐시 저장소가 준비돼 있다.\n",
        "  - 인메모리 캐시\n",
        "  - SQLite 캐시\n",
        "  - Redis캐시\n",
        "  -SQLAIchemy 캐시\n",
        "  "
      ],
      "metadata": {
        "id": "Wv8xfE1YlUlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 캐시 활성화\n",
        "LLM 애플리케이션을 개발하다보면 개발이나 테스트 단계에서 동일한 프롬프트로 반복해서 호출해야 하는 경우가 생긴다. 코딩을 하다가 에러가 나거나 아니면 테스트 결과를 보거나 할때는 동일 프롬프트로 동일 모델을 계속 호출하는데, 결과값은 거의 비슷하기 때문에, 계속해서 같은 질문을 호출하는 것은 비용이 낭비 된다. 같은 프롬프트라면 결과 값을 캐슁해놓고 개발에 사용해도 큰문제가 없다.\n",
        "Langchain에서는 동일(또는 유사) 프롬프트에 대해서 결과를 캐슁하여 API 호출을 줄일 수 있는 기능을 제공한다.  \n",
        "① 캐시 활성화\n",
        "- 캐시를 활성화하려면 langchain.llm_chain에 캐시를 지정한다."
      ],
      "metadata": {
        "id": "_dzYYCI_l2yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "from langchain.cache import InMemoryCache\n",
        "\n",
        "# 캐시 활성화\n",
        "langchain.llm_cache = InMemoryCache()"
      ],
      "metadata": {
        "id": "3oYdrzmmlUGI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "② LLM 호출\n",
        "- 응답의 llm_output에 정보가 있으므로 API를 호출한다."
      ],
      "metadata": {
        "id": "yfUE0_PZnAPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 첫 번째 LLM 호출 : 응답의 llm_output에 정보가 있으므로 API를 호출했음을 알 수 있다.\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TzpIvfdnQ5e",
        "outputId": "9300a77c-c62e-448f-fc91-82b40c62f4e9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파랑색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 16, 'total_tokens': 51}, 'model_name': 'text-davinci-003'}, run=[RunInfo(run_id=UUID('478d8508-1bbe-47b7-9ae0-327ebaef269f'))])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2번째 이후 LLM 호출 : 응답의 llm_output에 정보가 없으므로 캐시를 이용했음을 알 수 있다.\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTOdH-4InUAv",
        "outputId": "cac9a698-fdbe-4dd4-93b0-3092d073d49a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파랑색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={}, run=None)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3번째 이후 LLM 호출 : 응답의 llm_output에 정보가 없으므로 캐시를 이용했음을 알 수 있다.\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIk_Dfkrqwth",
        "outputId": "672c6b2b-91bb-4dbc-feb3-59d10b82a4fe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파랑색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={}, run=None)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 특정 LLM의 캐시 비활성화\n",
        "- 전역 캐시를 활성화한 경우 필요에 따라 특정 LLM의 캐시를 비활성화 할 수도 있다."
      ],
      "metadata": {
        "id": "-MUiex--nXLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 특정 LLM에 대한 메모리 캐시 비활성화\n",
        "llm = OpenAI(cache=False)"
      ],
      "metadata": {
        "id": "WhoSlh2cuIK2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM 호출\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOj9wQEvuJxo",
        "outputId": "08623b04-75e3-4013-d8cc-4cb052dce471"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파란색입니다. ', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 11, 'total_tokens': 29}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('e55a456e-025a-4b08-ac08-821e5bfa2cff'))])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM 호출\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suAVMdmfuNm5",
        "outputId": "93542760-da59-4e6a-f031-50b2ba8b415b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n파란색', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 11, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('e7c2b00e-0a57-42bd-b484-7d1f27b6b217'))])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전역 캐시 비활성화"
      ],
      "metadata": {
        "id": "hxGAhtxPudTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 전역 캐시 비활성화\n",
        "langchain.llm_cache = None"
      ],
      "metadata": {
        "id": "XnQ8lcPRuvTy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LLM 호출\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzL2PwNNuxrx",
        "outputId": "76ac8657-a1a3-43e4-f9e9-d5a545a01e23"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n하늘의 색깔은 파란색입니다.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 11, 'total_tokens': 28}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('944079fa-63af-47bd-8684-e12a1c0a83ee'))])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LLM 호출\n",
        "llm.generate([\"하늘의 색깔은?\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HR66T4-PuyQT",
        "outputId": "dcc5ed06-de1a-4d68-d23e-30e9a84e0529"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[Generation(text='\\n\\n파란색', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 11, 'total_tokens': 17}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('61d41a37-99d1-4c09-990e-114595e72b9b'))])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM의 비동기 처리\n",
        "랭체인은 asyncio 라이브러리를 이용해 LLM의 비동기 처리를 지원한다. 동기식 처리는 프로그램이 하나의 작업을 완료할 때까지 다른 작업을 시작하지 않는 방식이고, 비동기식 처리는 프로그램이 하나의 작업을 시작함과 동시에 다른 작업을 시작하는 방식이다."
      ],
      "metadata": {
        "id": "lBmVRzd8vC2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "① 동기화 처리로 10번의 호출에 걸린 시간을 측정"
      ],
      "metadata": {
        "id": "bdXoqKm4vl7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# 동기화 처리로 10번 호출하는 함수\n",
        "def generate_serially():\n",
        "    llm = OpenAI(temperature=0.9)\n",
        "    for _ in range(10):\n",
        "        resp = llm.generate([\"안녕하세요!\"])\n",
        "        print(resp.generations[0][0].text)\n",
        "\n",
        "\n",
        "# 시간 측정 시작\n",
        "s = time.perf_counter()\n",
        "\n",
        "# 동기화 처리로 10번 호출\n",
        "generate_serially()\n",
        "\n",
        "# 시간 측정 완료\n",
        "elapsed = time.perf_counter() - s\n",
        "print(f\"{elapsed:0.2f} 초\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zd5TV8RUvwBT",
        "outputId": "8d07ca60-477a-49bd-e9f3-3bfc63033cda"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 여러분의 언어에 대한 지식을 공유하면서 함께 대화하고 싶습니다. 저는 러시아에서 왔으며, 현재는 한국에서 외국어로서의 한국어를 가르치고 있습니다. 제 자신은 다른 언어들도 공부하며, 다른 문화를 배우는 것에 관심이 많습니다. 서로의 언어와 문화에 대해 이야기해보면서 서로 배우고 성장할 수 있는 좋은 기회가 되기를 바랍니다. 함께 언어를 공부하며 새로운 친구들을 만들어보는 것도 좋은 경험이 될 것 같습니다. 함께 공부해요~!\n",
            " 저는 한림대학교에서 액티브엑스를 사용한 자동화된 스크립트를 작성하고 개발하는 뛰어난 능력을 가진 최정호입니다. 많은 기업에서 일하는 동안 제가 수행한 업무들 중 하나는 웹페이지를 크롤링하여 데이터를 수집하고, 자동화된 작업을 수행하여 많은 시간과 노력을 아끼는 것입니다.\n",
            "\n",
            "저는 이를 위해 다양한 언어와 프레임워크를 숙달하고 있습니다. 예를 들어, C#, Python, Java, Selenium, BeautifulSoup 등을 자유롭게 다룰 수 있고, 액티브엑스를 사용한 환경에서도 적용 가능합니다. 또한 제가 일하는 동안에는 프로젝트 관리를 효율적\n",
            " 안녕하세요!\n",
            "\n",
            "\n",
            "안녕하세요! 반가워요.\n",
            " 저의 이름은 김영우입니다. 만나서 반가워요. 저는 대학에서 컴퓨터 공학을 전공하고 있고 취미로는 음악 듣기와 요리하기를 즐기고 있어요. 특히 어쿠스틱 기타를 연주하는 것을 좋아해요. 앞으로 재미있는 시간을 함께 보낼 수 있기를 기대해요. 언제든지 말씀해주세요! \n",
            " 저는 덕선입니다.\n",
            "남포동에 있는 성 덕수궁에서 태어나서 이름을 덕선이라고 합니다.\n",
            "저는 여행을 좋아하고, 다양한 나라를 여행하며 새로운 문화와 사람들을 만나는 것을 즐겨합니다.\n",
            "또한 역사와 문화, 예술 등에 관심이 많아서 많은 지식을 학습하고 있습니다.\n",
            "가족과 친구들과 함께하는 시간도 소중하게 생각하며 즐겁게 보내고 있습니다.\n",
            "앞으로도 계속해서 새로운 경험을 쌓고 세상을 더 많이 알아가는 것을 목표로 살고 있습니다.\n",
            "더 많은 사람들과 소통하고 교류하며 서로 배우\n",
            " 저는 김민희입니다. 반갑습니다.\n",
            "\n",
            "반가워요, 김민희님! 저는 [이름]이라고 해요. 함께 즐거운 시간 보내요.\n",
            "\n",
            "안녕하세요! 반갑습니다! \n",
            "안녕하세요! 반가워요!\n",
            " 홍찬기입니다.\n",
            "덕성여대에서 전자공학을 전공하고 있으며, 현재는 4학년을 다니고 있습니다.\n",
            "저는 인공지능에 큰 흥미를 느끼고 있으며, 전공 수업 이외에도 관련 공부와 프로젝트를 진행하고 있습니다.\n",
            "또한, 빅데이터 분석과 웹 개발에도 관심이 있어 관련 공부를 하고 있습니다.\n",
            "저는 새로운 기술과 분야에 대한 호기심이 많고, 문제 해결 능력을 중요하게 생각합니다. \n",
            "동료들과 함께 협력하고 소통하는 것을 즐기며, 책임감 있고 성실한 성격을 갖고 있습니다.\n",
            "앞으로 더 많은 경험을\n",
            " \n",
            " 공매니아입니다. 매물문의 감사드립니다.\n",
            "\n",
            "저희 매물의 위치나 상세사항 등을 알려드리기 위해 성함과 연락처를 남겨주시면 감사하겠습니다.\n",
            "\n",
            "또한, 궁금하신 점이나 추가로 알고 싶은 내용이 있으시면 언제든지 문의해주시면 성심성의껏 답변드리겠습니다.\n",
            "\n",
            "감사합니다. 좋은 하루 보내세요!\n",
            "17.10 초\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "② 비동기식으로 10번 호출에 걸린 시간을 측정"
      ],
      "metadata": {
        "id": "Qv4Rxi8FwPm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "# 이벤트 루프를 중첩하는 설정\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# 비동기 처리로 한 번만 호출하는 함수\n",
        "#agenerate이 메서드를 사용하여 OpenAI LLM을 비동기식으로 호출 할 수 있다.\n",
        "async def async_generate(llm):\n",
        "    resp = await llm.agenerate([\"안녕하세요!\"])\n",
        "    print(resp.generations[0][0].text)\n",
        "\n",
        "# 비동기 처리로 10회 호출하는 함수\n",
        "async def generate_concurrently():\n",
        "    llm = OpenAI(temperature=0.9)\n",
        "    tasks = [async_generate(llm) for _ in range(10)]\n",
        "    await asyncio.gather(*tasks)\n",
        "\n",
        "# 시간 측정 시작\n",
        "#파이썬 3.3+ 이상부터 perf_counter와 process_time를 사용할 수 있는데  차이점\n",
        "#perf_counter는 sleep 함수를 호출하여 대기한  시간을 포함하여 측정\n",
        "#process_time는 실제로 연산하는데 걸린 시간만 측정\n",
        "s = time.perf_counter()\n",
        "print('start:', s)\n",
        "\n",
        "# 비동기 처리로 10회 호출\n",
        "asyncio.run(generate_concurrently())\n",
        "\n",
        "# 시간 측정 완료\n",
        "elapsed = time.perf_counter() - s\n",
        "print(f\"{elapsed:0.2f} 초\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzbu2u_twYgV",
        "outputId": "3a054268-9e64-4acd-f8b2-1844bcb2a124"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start: 1016.727987898\n",
            "]\n",
            "\n",
            " 저는 소영이에요. 반가워요.\n",
            " 수정한 파일입니다\n",
            "\n",
            "\n",
            "안녕하세요! 반가워요! \n",
            "\n",
            " tFlas님!\n",
            "안녕하세요! 저도 반가워요! 잘 부탁드립니다~\n",
            " 저는 2020년 12월에 강남병원에서 뇌졸중으로 입원한 65살 이모입니다. 뇌졸중으로 인해 영양상태가 매우 안좋아지고 말초신경마비가 나타나 식사를 하는 것이 어려웠습니다. 하지만 강남병원에서 제공하는 영양관리 프로그램을 통해 건강한 식습관을 유지하면서 회복할 수 있었습니다.\n",
            "\n",
            "입원 당시에는 아침, 점심, 저녁 식사 시간에 영양사가 개별적으로 찾아와 식사를 도와주셨습니다. 처음에는 말씀을 못하고 먹는 것도 어려웠지만, 영양사님들의 친절한 도움과\n",
            ">\n",
            "\n",
            "안녕하세요! 오늘은 날씨가 매우 좋았어요. 엄마와 함께 공원에 가서 나들이를 하면서 여러 가지 놀이를 즐겼어요. 다슬이와 함께 차 안에서 노래를 들으며 신나는 시간도 가졌답니다. 그리고 공원에서는 저희 가족끼리 놀이공원에 들어가서 롤러코스터에서도 엄마와 함께 떠난 놀이를 즐기기도 했어요. 그리고 저녁에는 여러 가지 맛있는 음식도 먹으면서 맛있는 시간을 보냈어요. 오늘 하루도 너무 즐거웠고 내일도 또 다른 재미있는 일들이 기\n",
            " 똑똑한 인공지능 청소기 도우미, LG 디오스입니다.\n",
            "\n",
            "저희 디오스는 최첨단 기술을 적용하여 가정의 소중한 공간을 효율적으로 청소해드립니다. 먼지, 이물질을 감지하고 각종 바닥재에 맞는 적절한 청소 모드를 선택하여 최상의 성능을 발휘합니다.\n",
            "\n",
            "뿐만 아니라, 스마트폰 앱과 연동하여 원격으로 청소를 예약하고, 진행 상황을 실시간으로 확인할 수 있습니다. 또한, 소음을 줄이고 배터리 효율을 높인 저소음 모드와 급속 충전 기능을 지원하여 편리한 사용이 가능합니다.\n",
            "\n",
            "LG 디오스는 똑\n",
            " GOORM입니다.\n",
            "\n",
            "저희는 사용자들에게 다양한 편의를 제공하기 위해 노력하고 있습니다. 현재 사이트 내에서 소프트웨어 개발 및 데이터 분석 관련 실습 환경을 제공하고 있으며, 실시간으로 코딩하면서 바로 결과를 확인할 수 있도록 도와드리고 있습니다.\n",
            "\n",
            "또한 자유롭게 공유 및 협업할 수 있는 커뮤니티 기능을 제공하여 개발자들 간의 지식 공유와 네트워킹을 촉진하고 있습니다.\n",
            "\n",
            "GOORM은 더 나은 서비스를 제공하기 위해 끊임없이 노력할 것이며, 더 많은 사용자들의 도움으로 더 큰 가치를 만들어 나갈 것입니다.\n",
            "\n",
            "감사합니다.\n",
            " [빅데이터 입문반 2기] 입니다.\n",
            "\n",
            "저희 수업에 참여해 주셔서 감사합니다. 빅데이터 분석을 배우는 것은 지금과 미래 모두에게 매우 중요한 스킬입니다.\n",
            "\n",
            "저희가 준비한 빅데이터 입문반 2기 프로그램은 여러분을 비즈니스 분야에서 빅데이터를 객관적이고 전문적으로 분석할 수 있는 수준까지 함께 이끌어 드리는 것이 목표입니다.\n",
            "\n",
            "이 프로그램을 통해 여러분은 빅데이터 분석에 대한 이해를 넓히고, 실제 업무에 적용할 수 있는 경험을 쌓을 수 있을 것입니다. 함께 열심히 공부하고 성장해 나가는\n",
            " 컴퓨터 정보과 박연기 입니다. 컴퓨터 정보과는 컴퓨터와 소프트웨어, 네트워크 등의 기초 이론과 응용 분야를 학습하는 과목입니다.\n",
            "\n",
            "컴퓨터 정보과에서는 컴퓨터 프로그래밍 언어를 배우고, 네트워크 구성 및 관리, 데이터베이스 시스템, 알고리즘 등의 다양한 개념을 학습합니다. 이를 통해 컴퓨터 시스템을 이해하고 구축하는 기술을 습득하고, 문제 해결 능력도 함양합니다.\n",
            "\n",
            "컴퓨터 정보과에서는 이론적인 학습뿐만 아니라 실제 프로젝트를 수행하며 실무 능력을 키우는 것에도 중\n",
            "3.14 초\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM 스트리밍\n",
        "- LLM의 스트리밍은 한 번에 모두 출력하지 않고 토큰 단위로 출력을 돌려줌으로써 체감하는 대기 시간을 줄이는 기능이다.\n",
        "- 스트리밍을 이용하려면 LLM클래스의 streaming 매개변수에 True을 지정하고, callbacks에 CallbackHandler를 지정한다. 표준출력으로 스트리밍하는 StreamingStdOutCallbackHandler를 지정한다."
      ],
      "metadata": {
        "id": "obmBxBOKzm1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "# 스트리밍 방식으로 출력할 LLM을 준비\n",
        "llm = OpenAI(\n",
        "    streaming=True,\n",
        "    callbacks=[StreamingStdOutCallbackHandler()],\n",
        "    verbose=True,\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# LLM 호출\n",
        "resp = llm(\"즐거운 ChatGPT 생활을 가사로 만들어 주세요.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1OzyDh50Krz",
        "outputId": "2689ed36-ad63-4c33-92fc-aaf52e268270"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ChatGPT, 너는 나의 친구\n",
            "매일 나를 즐겁게 해주는\n",
            "나의 소중한 존재\n",
            "마치 마법처럼 나를 웃게 해주는\n",
            "너의 말 한마디에 나는 행복해져\n",
            "우리 함께하는 시간은 너무 소중해\n",
            "나의 모든 이야기를 들어주는\n",
            "나의 가장 신뢰할 수 있는 친구\n",
            "ChatGPT, 너와 함께라면\n",
            "나는 언제나 행복할 수 있어\n",
            "감사해, 나의 사랑하는 ChatGPT"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}